# -*- coding: utf-8 -*-
"""web scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4F9EvNYPFCUex6BkNgdEw27gSIpD4Ft
"""

#install the Pyterrier framework
!pip install python-terrier
# install the nltk modules
!pip install nltk

# Import Libraries

import pyterrier as pt
import re
import pandas as pd
import nltk
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

from nltk.stem import *
from nltk.stem.porter import *
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

pd.set_option('display.max_colwidth', 150)
nltk.download('punkt')

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
print(stopwords.words('english'))

# Initialize Porter stemmer
stemmer = PorterStemmer()

def Steem_text(text):

    tokens = word_tokenize(text)
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    # print (tokens)
    return ' '.join(stemmed_tokens)


#a function to clean the tweets
def clean(text):
   text = re.sub(r"http\S+", " ", text) # remove urls
   text = re.sub(r"RT ", " ", text) # remove rt
   text = re.sub(r"@[\w]*", " ", text) # remove handles
   text = re.sub(r"[\.\,\#_\|\:\?\?\/\=]", " ", text) # remove special characters
   text = re.sub(r'\t', ' ', text) # remove tabs
   text = re.sub(r'\n', ' ', text) # remove line jump
   text = re.sub(r"\s+", " ", text) # remove extra white space
   text = text.strip()
   return text



def remove_stopwords(text):

    tokens = word_tokenize(text)
    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words] #Lower is used to normalize al the words make them in lower case
    print('Tokens are:',tokens,'\n')
    return ' '.join(filtered_tokens)



#we need to process the query also as we did for documents
def preprocess(sentence):
  sentence = remove_stopwords(sentence)
  sentence = clean(sentence)
  sentence = Steem_text(sentence)

  return sentence

tweets = pd.read_csv('/content/twitter_dataset.csv')
tweets

#the docno will be our tweetID
tweets["docno"]=tweets["Tweet_ID"].astype(str)
tweets[["docno"]]

import nltk
nltk.download('punkt_tab')

# Display the  processed DataFrames
tweets['processed_text'] = tweets['Text'].apply(preprocess)

print('dataFrame after processing:\n')
tweets

indexer = pt.DFIndexer("./DatasetIndex", overwrite=True)
# index the text, record the docnos as metadata
index_ref = indexer.index(tweets["processed_text"], tweets["docno"])
print(index_ref.toString())

index = pt.IndexFactory.of(index_ref)

# Retrieve collection statistics using PyTerrier
stats = index.getCollectionStatistics()
print(stats.toString())

meta = index.getMetaIndex()
inv = index.getInvertedIndex()
lex = index.getLexicon()

unique_terms = {}
# mapping unique terms
for kv in index.getLexicon():
    term = kv.getKey()
    le = index.getLexicon().getLexiconEntry(term)
    docnos = [meta.getItem("docno", posting.getId()) for posting in inv.getPostings(le)]
    if le.getDocumentFrequency() == 1:
      unique_terms[term] = docnos

unique_terms

for kv in index.getLexicon():
  print("%s -> %s " % (kv.getKey(), kv.getValue().toString()))

def preprocess(sentence):
  sentence = remove_stopwords(sentence)
  sentence = clean(sentence)
  sentence = Steem_text(sentence)

  return sentence

# # Need to install additional terrier package for PRF. It will take around 1 min
# !git clone https://github.com/terrierteam/terrier-prf/
# !apt-get install maven   #used for Java projects to manage project dependencies and build processes
# %cd /content/terrier-prf/
# !mvn install
# !pwd
# %cd ..

"""Query processing"""

# Matches documents that contain all tokens from a query and prints the matched document texts.
def match_docs(query, tweets):
  tokens = query.split()
  res = list()
  for id in tweets['docno']:
    counter = 0
    for token in tokens:
      if token in str(tweets[tweets['docno']== str(id)]['processed_text']):
        counter = counter + 1
      if counter == len(tokens):
        res.append(id)
  for idx in range(len(res)):
    print(f'Text:', str(tweets[tweets['docno']== str(res[idx])]['processed_text']))

  return res

# Loading the index
index = pt.IndexFactory.of(index_ref)

tf_idf = pt.BatchRetrieve(index, wmodel="TF_IDF")

query = 'job'
results=tf_idf.search(query)
results

# Retrieves and prints document IDs and frequency of preprocessed terms from the index.
meta = index.getMetaIndex()
inv = index.getInvertedIndex()
lex = index.getLexicon()

# List of terms to retrieve document frequency
terms = preprocess("Guess the job save")  # Add your terms here

for term in terms.split():
    le = lex.getLexiconEntry(term)
    if le is None:
        print(f"Term '{term}' not found in the lexicon.")
        continue

    # Access the inverted index posting list for the term
    for posting in inv.getPostings(le):
        docno = meta.getItem("docno", posting.getId())
        print(f"Document: {docno}, Frequency: {posting.getFrequency()} for term: {term}")

"""# Query Expansion"""

import pandas as pd
import pyterrier as pt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import os

# Define our retrieval model
bm25 = pt.BatchRetrieve(index, wmodel="BM25",num_results=10)

result = bm25.search(query)
result

tweets[['Text']][tweets['docno'].isin(results['docno'].loc[0:4].tolist())]

query = preprocess("year")
TF_IDF = pt.BatchRetrieve(index, wmodel="TF_IDF",num_results=10)
resultss = TF_IDF.search(query)
resultss

rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)

#output of the BM25 will be fed into the RM3 expander for query expansion.
rm3_qe = bm25 >> rm3_expander
expanded_query = rm3_qe.search(query).iloc[0]["query"]

expanded_query

# Just print the expanded query with term scores
for s in expanded_query.split()[1:]:
  print(s)
print("\n" + query)

# After that you can search using the expanded query
expanded_query_formatted = ' '.join(expanded_query.split()[1:])

results_wqe = bm25.search(expanded_query_formatted)

print("   Before Expansion    After Expansion")
print(pd.concat([results[['docid','score']][0:5].add_suffix('_1'),
            results_wqe[['docid','score']][0:5].add_suffix('_2')], axis=1).fillna(''))

#Let's check the tweets text for the top 5 retrieved tweets
tweets[['Text']][tweets['docno'].isin(results_wqe['docno'].loc[0:5].tolist())]

"""# Embeding"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd

# Function to calculate cosine similarity
def cosine_similarity(v1, v2):
    # Ensure the embeddings are summed across all tokens and reduced to a 1D array
    sum1 = np.sum(v1, axis=0)
    sum2 = np.sum(v2, axis=0)
    if sum1.ndim > 1:
        sum1 = np.sum(sum1, axis=0)
    if sum2.ndim > 1:
        sum2 = np.sum(sum2, axis=0)

    dot_product = np.dot(sum1, sum2)
    norm_v1 = np.linalg.norm(sum1)
    norm_v2 = np.linalg.norm(sum2)
    return dot_product / (norm_v1 * norm_v2)



import tensorflow_hub as hub
import tensorflow as tf
from transformers import BertTokenizer, BertModel

# Load ELMo
elmo = hub.load("https://tfhub.dev/google/elmo/3")

# Load BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

query = 'write'
similar = []
for doc_content in tweets['processed_text'][tweets['docno'].isin(results['docno'].loc[0:9].tolist())]:  # Process only the first 10 documents
    # Generate ELMo embeddings for the query sentence and the document content
    query_embedding = elmo.signatures["default"](tf.constant([query]))["elmo"].numpy()
    doc_embedding = elmo.signatures["default"](tf.constant([doc_content]))["elmo"].numpy()

    # Calculate cosine similarity between the query and the document
    similarity_score = cosine_similarity(query_embedding, doc_embedding)
    similar.append(similarity_score)

# Create a DataFrame to store similarity scores with corresponding documents
similarity_df = pd.DataFrame({
    'Document': tweets['processed_text'][tweets['docno'].isin(results['docno'].loc[0:9].tolist())],  # Include only the first 10 documents
    'Similarity Score': similar
})

# Rank the documents based on similarity scores
similarity_df = similarity_df.sort_values(by='Similarity Score', ascending=False)

# Print the ranked DataFrame
print(similarity_df)

"""# user interface"""

!pip install pyngrok
!pip install flask flask-ngrok pandas pytrec_eval

from flask import Flask, request, render_template_string
from pyngrok import ngrok

tokens2 =  tweets['processed_text'].apply(word_tokenize)
from flask import Flask, request, render_template_string
from pyngrok import ngrok
import pandas as pd

from nltk.tokenize import word_tokenize



# Flask App Initialization
app = Flask(__name__)


ngrok.set_auth_token('2wXnbxYdV3n3BxjgusI7FbfYCIv_3Dh5ZjZ12QANKbHi6Vjpj')
public_url = ngrok.connect(8000)
print('NGROK Tunnel URL:', public_url)

# HTML Template
def preprocess(text):
    # Example: simple case normalization and stripping extra whitespace
    return text.lower().strip()



# HTML Template for the home and search results page, styled to look like Google
HTML_TEMPLATE = '''
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Search Engine</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      text-align: center;
      background-color: #e6ecf0; /* Soft blue background */
    }
    header {
      background-color: #007bff; /* Vivid blue for header */
      color: white; /* White text for contrast */
      padding: 20px 0;
      border-bottom: 2px solid #0056b3; /* Darker blue border */
      position: fixed;
      width: 100%;
      top: 0;
      left: 0;
      right: 0;
      z-index: 100;
    }
    form {
      margin: 130px auto 20px;
      padding: 20px;
      width: 40%;
      box-shadow: 0px 2px 10px rgba(0,0,0,0.1); /* Shadow for depth */
    }
    input[type="text"] {
      width: 70%;
      height: 44px;
      padding: 10px 20px;
      font-size: 16px;
      border: 1px solid #0056b3; /* Consistent blue border */
      background-color: #fff; /* White background for input */
      color: #333; /* Dark text for readability */
      margin-bottom: 20px;
      border-radius: 22px;
      box-shadow: inset 0px 1px 3px rgba(0,0,0,0.1); /* Inner shadow for inset effect */
    }
    input[type="submit"] {
      height: 44px;
      width: 20%;
      background-color: #28a745; /* Green for action buttons */
      font-size: 16px;
      border: none;
      border-radius: 22px;
      cursor: pointer;
      color: white;
      transition: background-color 0.2s; /* Smooth transition for hover effect */
    }
    input[type="submit"]:hover {
      background-color: #218838; /* Darker green on hover */
    }
    ul {
      list-style: none;
      padding: 0;
      margin-top: 160px; /* Make space for fixed header */
      text-align: left;
      position: relative;
    }
    li {
      background-color: #fff; /* White background for list items */
      color: #333; /* Dark text for readability */
      margin: 10px auto;
      padding: 20px;
      border: 1px solid #ccc; /* Light gray border */
      width: 40%;
      box-shadow: 0px 4px 8px rgba(0,0,0,0.05); /* Soft shadow for 3D effect */
    }
    a {
      text-decoration: none;
      color: #007bff; /* Blue for links */
      font-size: 18px;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <header>
    <h1>Search Engine</h1>
    <form action="/search" method="post">
      <input type="text" name="query" placeholder="Enter search terms...">
      <input type="submit" value="Search">
    </form>
  </header>
  <ul>
    {% if query %}
      <h2 style="margin: 140px 0 20px; color: #333;">Results for: "{{ query }}"</h2>
      {% if results %}
        {% for doc, link, text in results %}
          <li><a href="{{ link }}">{{ text }} (Document: {{ doc }})</a></li>
        {% endfor %}
      {% else %}
        <p>No results found.</p>
      {% endif %}
    {% endif %}
  </ul>
</body>
</html>


'''

@app.route("/", methods=["GET"])
def home():
    return render_template_string(HTML_TEMPLATE)

@app.route("/search", methods=["POST"])
def search():
    query = request.form['query']
    search_results = []

    # Assuming an index is created and available
    tfidf_retriever = pt.BatchRetrieve(index, controls={"wmodel": "TF_IDF"}, num_results=30)
    query = preprocess(query)  # Ensure the query is preprocessed
    tfidf_results = tfidf_retriever.transform(query)

    # Handle results
    for idx, row in tfidf_results.iterrows():
        docno = row['docno']
        text = tweets[tweets['docno'] == docno]['Text'].values[0]
        link = f"http://example.com/doc/{docno}"  # Placeholder for actual link generation
        search_results.append((docno, link, text))

    return render_template_string(HTML_TEMPLATE, query=query, results=search_results)

if __name__ == "__main__":
    app.run(port=8000)